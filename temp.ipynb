{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose(transforms=[\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5), std=(0.5))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.EMNIST(root=\"./data\", split='mnist', train=True, download=True, transform=transformer)\n",
    "trainloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "test_dataset = torchvision.datasets.EMNIST(root=\"./data\", split='mnist', train=False, download=True, transform=transformer)\n",
    "testloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, trainloader, number_of_epochs=10, device='cpu'):\n",
    "    steps_per_epoch = len(trainloader)\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(number_of_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for (inputs, labels) in trainloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward + optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'[{epoch + 1}] loss: {running_loss / steps_per_epoch:.3f}')\n",
    "\n",
    "    print('Finished training')\n",
    "\n",
    "\n",
    "def test_model(model, dataloader):\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "\n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (images, labels) in dataloader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            n_total += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            result = n_correct / n_total * 100\n",
    "\n",
    "    print(f\"Accuracy on test set: {result:.1f}%\")\n",
    "\n",
    "\n",
    "def model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    size_all_kb = (param_size + buffer_size) / 1024\n",
    "    return 'model size: {:.3f}KB'.format(size_all_kb)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    # in_channels = 1, because nn working with grayscale images\n",
    "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=256, kernel_size=3)\n",
    "    self.conv2 = nn.Conv2d(in_channels=256, out_channels=64, kernel_size=3) # 64 (1/25)\n",
    "    # out_channels = number of filters\n",
    "    \n",
    "    self.relu = nn.ReLU()\n",
    "    self.relu1 = nn.ReLU()\n",
    "    self.relu2 = nn.ReLU()\n",
    "    \n",
    "    # in_features = conv2.out_channels * 5 * 5\n",
    "    # in_features = 64 * 5 * 5 = 1600\n",
    "    self.linear1 = nn.Linear(in_features=1600, out_features = 500) # 252x28 \\\\ before 700 was 28*3*3\n",
    "    self.linear2 = nn.Linear(in_features=500, out_features = 250)\n",
    "    self.linear3 = nn.Linear(in_features=250, out_features=10) # out_features = number of classes\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    x = self.relu1(self.conv1(x))\n",
    "    # x = F.relu(self.conv1(x)) # same as above\n",
    "    x = self.pool(x)\n",
    "\n",
    "    x = self.relu2(self.conv2(x))\n",
    "    # x = F.relu(self.conv2(x)) # same as above\n",
    "    x = self.pool(x)\n",
    "\n",
    "    x = torch.flatten(x, 1) \n",
    "    x = self.relu(self.linear1(x))\n",
    "    # x = F.relu(self.linear1(x)) # same as above\n",
    "    x = self.relu(self.linear2(x))\n",
    "    x = self.linear3(x)\n",
    "    return x\n",
    "    \n",
    "model = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(\n",
    "    model=model,\n",
    "    criterion = nn.CrossEntropyLoss(),\n",
    "    optimizer=optim.Adam(params=model.parameters(), lr=0.001),\n",
    "    trainloader=trainloader,\n",
    "    number_of_epochs=10,\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(\n",
    "    model=model, \n",
    "    dataloader=testloader\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAT_ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=256, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=256, out_channels=64, kernel_size=3)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # in_features = 64 * 6 * 6 = 1600\n",
    "        self.linear1 = nn.Linear(in_features=1600, out_features = 500) # 252x28 \\\\ before 700 was 28*3*3\n",
    "        self.linear2 = nn.Linear(in_features=500, out_features = 250)\n",
    "        self.linear3 = nn.Linear(in_features=250, out_features=10) # out_features = number of classes\n",
    "\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.quant(x)\n",
    "        \n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        \n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# create a model instance\n",
    "model_fp32 = QAT_ConvNet().to(device)\n",
    "\n",
    "# model must be set to eval for fusion to work\n",
    "model_fp32.eval()\n",
    "\n",
    "# attach a global qconfig, which contains information about what kind\n",
    "# of observers to attach. Use 'fbgemm' for server inference and\n",
    "# 'qnnpack' for mobile inference. Other quantization configurations such\n",
    "# as selecting symmetric or assymetric quantization and MinMax or L2Norm\n",
    "# calibration techniques can be specified here.\n",
    "model_fp32.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "\n",
    "# fuse the activations to preceding layers, where applicable\n",
    "# this needs to be done manually depending on the model architecture\n",
    "model_fp32_fused = torch.quantization.fuse_modules(\n",
    "    model_fp32, [['conv1', 'relu1']])\n",
    "model_fp32_fused = torch.quantization.fuse_modules(\n",
    "    model_fp32_fused, [['conv2', 'relu2']])\n",
    "\n",
    "\n",
    "# Prepare the model for QAT. This inserts observers and fake_quants in\n",
    "# the model needs to be set to train for QAT logic to work\n",
    "# the model that will observe weight and activation tensors during calibration.\n",
    "model_fp32_prepared = torch.quantization.prepare_qat(model_fp32_fused.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training_loop(model_fp32_prepared)\n",
    "train_model(\n",
    "    model=model_fp32_prepared,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    optimizer=optim.Adam(params=model_fp32_prepared.parameters(), lr=0.001),\n",
    "    trainloader=trainloader,\n",
    "    number_of_epochs=10,\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# quantizes the weights, computes and stores the scale and bias value to be\n",
    "# used with each activation tensor, fuses modules where appropriate,\n",
    "# and replaces key operators with quantized implementations.\n",
    "\n",
    "model_fp32_prepared = model_fp32_prepared.to('cpu')\n",
    "model_fp32_prepared.eval()\n",
    "model_int8 = torch.quantization.convert(model_fp32_prepared)\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "# res = model_int8(input_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function crashed Google Colab (because of calculation on cuda in testing function)\n",
    "test_model(\n",
    "    model=model_int8, \n",
    "    dataloader=testloader\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Size of basic model: {model_size(model)}\")\n",
    "print(f\"Size of quantized model: {model_size(model_int8)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict one random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = int(torch.randint(low=0, high=10000, size=(1,)))\n",
    "x, y_true = test_dataset[index]\n",
    "print(x.shape)\n",
    "x = x.reshape([28, 28])\n",
    "print(x.shape)\n",
    "\n",
    "# print(y_true, make_prediction(model, x))\n",
    "print(f\"Correct answer: {y_true}, Predicted: {make_prediction(model_int8, x)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.9 (tags/v3.9.9:ccb0e6a, Nov 15 2021, 18:08:50) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97da180fb1c9398baab8270cb4abaf1f321012feba30ee9bb8b7a4ed1d73de03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
