{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9QXsrr6Mp5e_",
        "1EWDw3bip8Ie",
        "vFM8UV9CreIX",
        "xXkTAJ9ws1Y6",
        "OgkWg605tE1y",
        "OBt0WDzyujnk",
        "xC96eesMqYo-",
        "mszPTrYOluym",
        "VTHK-wAWV57B"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QXsrr6Mp5e_"
      },
      "source": [
        "# MNIST Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBGOnz5NpiTw"
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, mnist=True):\n",
        "      \n",
        "        super(Net, self).__init__()\n",
        "        if mnist:\n",
        "          num_channels = 1\n",
        "        else:\n",
        "          num_channels = 3\n",
        "          \n",
        "        self.conv1 = nn.Conv2d(num_channels, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        if mnist:\n",
        "          self.fc1 = nn.Linear(4*4*50, 500)\n",
        "          self.flatten_shape = 4*4*50\n",
        "        else:\n",
        "          self.fc1 = nn.Linear(1250, 500)\n",
        "          self.flatten_shape = 1250\n",
        "\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "        \n",
        "      \n",
        "    def forward(self, x, vis=False, axs=None):\n",
        "        X = 0\n",
        "        y = 0\n",
        "\n",
        "        if vis:\n",
        "          axs[X,y].set_xlabel('Entry into network, input distribution visualised below: ')\n",
        "          visualise(x, axs[X,y])\n",
        "\n",
        "          axs[X,y+1].set_xlabel(\"Visualising weights of conv 1 layer: \")\n",
        "          visualise(self.conv1.weight.data, axs[X,y+1])\n",
        "\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "\n",
        "        if vis:\n",
        "          axs[X,y+2].set_xlabel('Output after conv1 visualised below: ')\n",
        "          visualise(x,axs[X,y+2])\n",
        "\n",
        "          axs[X,y+3].set_xlabel(\"Visualising weights of conv 2 layer: \")\n",
        "          visualise(self.conv2.weight.data, axs[X,y+3])\n",
        "\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "\n",
        "        if vis:\n",
        "          axs[X,y+4].set_xlabel('Output after conv2 visualised below: ')\n",
        "          visualise(x,axs[X,y+4])\n",
        "\n",
        "          axs[X+1,y].set_xlabel(\"Visualising weights of fc 1 layer: \")\n",
        "          visualise(self.fc1.weight.data, axs[X+1,y])\n",
        "\n",
        "        x = F.max_pool2d(x, 2, 2)  \n",
        "        x = x.view(-1, self.flatten_shape)\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        if vis:\n",
        "          axs[X+1,y+1].set_xlabel('Output after fc1 visualised below: ')\n",
        "          visualise(x,axs[X+1,y+1])\n",
        "\n",
        "          axs[X+1,y+2].set_xlabel(\"Visualising weights of fc 2 layer: \")\n",
        "          visualise(self.fc2.weight.data, axs[X+1,y+2])\n",
        "\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        if vis:\n",
        "          axs[X+1,y+3].set_xlabel('Output after fc2 visualised below: ')\n",
        "          visualise(x,axs[X+1,y+3])\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "    "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'"
      ],
      "metadata": {
        "id": "1dWKfQ9Q70vH"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EWDw3bip8Ie"
      },
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujzd_d1kp_sX",
        "outputId": "09004e91-07c7-4158-c627-17981fd6e37c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "   \n",
        "        if batch_idx % args[\"log_interval\"] == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(args, model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "def main(mnist=True):\n",
        " \n",
        "    batch_size = 64\n",
        "    test_batch_size = 64\n",
        "    epochs = 10\n",
        "    lr = 0.01\n",
        "    momentum = 0.5\n",
        "    seed = 1\n",
        "    log_interval = 500\n",
        "    save_model = False\n",
        "    no_cuda = False\n",
        "    \n",
        "    use_cuda = not no_cuda and torch.cuda.is_available()\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "    if mnist:\n",
        "      train_loader = torch.utils.data.DataLoader(\n",
        "          datasets.MNIST('../data', train=True, download=True,\n",
        "                        transform=transforms.Compose([\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize((0.1307,), (0.3081,))\n",
        "                        ])),\n",
        "          batch_size=batch_size, shuffle=True, **kwargs)\n",
        "      \n",
        "      test_loader = torch.utils.data.DataLoader(\n",
        "          datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize((0.1307,), (0.3081,))\n",
        "                        ])),\n",
        "          batch_size=test_batch_size, shuffle=True, **kwargs)\n",
        "    else:\n",
        "      transform = transforms.Compose(\n",
        "          [transforms.ToTensor(),\n",
        "          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "      trainset = datasets.CIFAR10(root='./dataCifar', train=True,\n",
        "                                              download=True, transform=transform)\n",
        "      train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                                shuffle=True, num_workers=2)\n",
        "\n",
        "      testset = datasets.CIFAR10(root='./dataCifar', train=False,\n",
        "                                            download=True, transform=transform)\n",
        "      test_loader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size,\n",
        "                                              shuffle=False, num_workers=2)\n",
        "          \n",
        "  \n",
        "    model = Net(mnist=mnist).to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "    args = {}\n",
        "    args[\"log_interval\"] = log_interval\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(args, model, device, train_loader, optimizer, epoch)\n",
        "        test(args, model, device, test_loader)\n",
        "\n",
        "    if (save_model):\n",
        "        torch.save(model.state_dict(),\"mnist_cnn.pt\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = main()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.299172\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.156517\n",
            "\n",
            "Test set: Average loss: 0.1058, Accuracy: 9676/10000 (97%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.209260\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.161600\n",
            "\n",
            "Test set: Average loss: 0.0683, Accuracy: 9782/10000 (98%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.028544\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.070974\n",
            "\n",
            "Test set: Average loss: 0.0482, Accuracy: 9841/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.019385\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.027914\n",
            "\n",
            "Test set: Average loss: 0.0410, Accuracy: 9858/10000 (99%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.092989\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.034980\n",
            "\n",
            "Test set: Average loss: 0.0368, Accuracy: 9877/10000 (99%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.080683\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.012147\n",
            "\n",
            "Test set: Average loss: 0.0390, Accuracy: 9876/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.011660\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.021835\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 9886/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.039075\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.003043\n",
            "\n",
            "Test set: Average loss: 0.0438, Accuracy: 9850/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.008857\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.023365\n",
            "\n",
            "Test set: Average loss: 0.0296, Accuracy: 9903/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.007935\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.047993\n",
            "\n",
            "Test set: Average loss: 0.0329, Accuracy: 9891/10000 (99%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDkkrT2prCU9"
      },
      "source": [
        "# Quantization of Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFM8UV9CreIX"
      },
      "source": [
        "## Quantization Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCsoFvwLrgdu"
      },
      "source": [
        "from collections import namedtuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "QTensor = namedtuple('QTensor', ['tensor', 'scale', 'zero_point'])\n",
        "\n",
        "def calcScaleZeroPoint(min_val, max_val,num_bits=8):\n",
        "  # Calc Scale and zero point of next \n",
        "  qmin = 0.\n",
        "  qmax = 2.**num_bits - 1.\n",
        "\n",
        "  scale = (max_val - min_val) / (qmax - qmin)\n",
        "\n",
        "  initial_zero_point = qmin - min_val / scale\n",
        "  \n",
        "  zero_point = 0\n",
        "  if initial_zero_point < qmin:\n",
        "      zero_point = qmin\n",
        "  elif initial_zero_point > qmax:\n",
        "      zero_point = qmax\n",
        "  else:\n",
        "      zero_point = initial_zero_point\n",
        "\n",
        "  zero_point = int(zero_point)\n",
        "\n",
        "  return scale, zero_point\n",
        "\n",
        "def calcScaleZeroPointSym(min_val, max_val,num_bits=8):\n",
        "  \n",
        "  # Calc Scale \n",
        "  max_val = max(abs(min_val), abs(max_val))\n",
        "  qmin = 0.\n",
        "  qmax = 2.**(num_bits-1) - 1.\n",
        "\n",
        "  scale = max_val / qmax\n",
        "\n",
        "  return scale, 0\n",
        "\n",
        "def quantize_tensor(x, num_bits=8, min_val=None, max_val=None):\n",
        "    \n",
        "    if not min_val and not max_val: \n",
        "      min_val, max_val = x.min(), x.max()\n",
        "\n",
        "    qmin = 0.\n",
        "    qmax = 2.**num_bits - 1.\n",
        "\n",
        "    scale, zero_point = calcScaleZeroPoint(min_val, max_val, num_bits)\n",
        "    q_x = zero_point + x / scale\n",
        "    q_x.clamp_(qmin, qmax).round_()\n",
        "    q_x = q_x.round().byte()\n",
        "    \n",
        "    return QTensor(tensor=q_x, scale=scale, zero_point=zero_point)\n",
        "\n",
        "def dequantize_tensor(q_x):\n",
        "    return q_x.scale * (q_x.tensor.float() - q_x.zero_point)\n",
        "\n",
        "def quantize_tensor_sym(x, num_bits=8, min_val=None, max_val=None):\n",
        "    \n",
        "    if not min_val and not max_val: \n",
        "      min_val, max_val = x.min(), x.max()\n",
        "\n",
        "    max_val = max(abs(min_val), abs(max_val))\n",
        "    qmin = 0.\n",
        "    qmax = 2.**(num_bits-1) - 1.\n",
        "\n",
        "    scale = max_val / qmax   \n",
        "\n",
        "    q_x = x/scale\n",
        "\n",
        "    q_x.clamp_(-qmax, qmax).round_()\n",
        "    q_x = q_x.round()\n",
        "    return QTensor(tensor=q_x, scale=scale, zero_point=0)\n",
        "\n",
        "def dequantize_tensor_sym(q_x):\n",
        "    return q_x.scale * (q_x.tensor.float())"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXkTAJ9ws1Y6"
      },
      "source": [
        "## Rework Forward pass of Linear and Conv Layers to support Quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5xNLrchrI6u"
      },
      "source": [
        "def quantizeLayer(x, layer, stat, scale_x, zp_x, vis=False, axs=None, X=None, y=None, sym=False, num_bits=8):\n",
        "  # for both conv and linear layers\n",
        "\n",
        "  # cache old values\n",
        "  W = layer.weight.data\n",
        "  B = layer.bias.data\n",
        "\n",
        "  # WEIGHTS SIMULATED QUANTIZED\n",
        "\n",
        "  # quantize weights, activations are already quantized\n",
        "  if sym:\n",
        "    w = quantize_tensor_sym(layer.weight.data,num_bits=num_bits) \n",
        "    b = quantize_tensor_sym(layer.bias.data,num_bits=num_bits)\n",
        "  else:\n",
        "    w = quantize_tensor(layer.weight.data, num_bits=num_bits) \n",
        "    b = quantize_tensor(layer.bias.data, num_bits=num_bits)\n",
        "\n",
        "  layer.weight.data = w.tensor.float()\n",
        "  layer.bias.data = b.tensor.float()\n",
        "\n",
        "  ## END WEIGHTS QUANTIZED SIMULATION\n",
        "\n",
        "\n",
        "  if vis:\n",
        "    axs[X,y].set_xlabel(\"Visualising weights of layer: \")\n",
        "    visualise(layer.weight.data, axs[X,y])\n",
        "\n",
        "  # QUANTIZED OP, USES SCALE AND ZERO POINT TO DO LAYER FORWARD PASS. (How does backprop change here ?)\n",
        "  # This is Quantization Arithmetic\n",
        "  scale_w = w.scale\n",
        "  zp_w = w.zero_point\n",
        "  scale_b = b.scale\n",
        "  zp_b = b.zero_point\n",
        "  \n",
        "  if sym:\n",
        "    scale_next, zero_point_next = calcScaleZeroPointSym(min_val=stat['min'], max_val=stat['max'])\n",
        "  else:\n",
        "    scale_next, zero_point_next = calcScaleZeroPoint(min_val=stat['min'], max_val=stat['max'])\n",
        "\n",
        "  # Preparing input by saturating range to num_bits range.\n",
        "  if sym:\n",
        "    X = x.float()\n",
        "    layer.weight.data = ((scale_x * scale_w) / scale_next)*(layer.weight.data)\n",
        "    layer.bias.data = (scale_b/scale_next)*(layer.bias.data)\n",
        "  else:\n",
        "    X = x.float() - zp_x\n",
        "    layer.weight.data = ((scale_x * scale_w) / scale_next)*(layer.weight.data - zp_w)\n",
        "    layer.bias.data = (scale_b/scale_next)*(layer.bias.data + zp_b)\n",
        "\n",
        "  # All int computation\n",
        "  if sym:  \n",
        "    x = (layer(X)) \n",
        "  else:\n",
        "    x = (layer(X)) + zero_point_next \n",
        "  \n",
        "  # cast to int\n",
        "  x.round_()\n",
        "\n",
        "  # Perform relu too\n",
        "  x = F.leaky_relu(x)\n",
        "\n",
        "  # Reset weights for next forward pass\n",
        "  layer.weight.data = W\n",
        "  layer.bias.data = B\n",
        "  \n",
        "  return x, scale_next, zero_point_next\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgkWg605tE1y"
      },
      "source": [
        "## Get Stats for Quantizing Activations of Network.\n",
        "\n",
        "This is done by running the network with around 1000 examples and getting the average min and max activation values before and after each layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GecOkNLhtVh9"
      },
      "source": [
        "# Get Min and max of x tensor, and stores it\n",
        "def updateStats(x, stats, key):\n",
        "  max_val, _ = torch.max(x, dim=1)\n",
        "  min_val, _ = torch.min(x, dim=1)\n",
        "\n",
        "  # add ema calculation\n",
        "\n",
        "  if key not in stats:\n",
        "    stats[key] = {\"max\": max_val.sum(), \"min\": min_val.sum(), \"total\": 1}\n",
        "  else:\n",
        "    stats[key]['max'] += max_val.sum().item()\n",
        "    stats[key]['min'] += min_val.sum().item()\n",
        "    stats[key]['total'] += 1\n",
        "  \n",
        "  weighting = 2.0 / (stats[key]['total']) + 1\n",
        "\n",
        "  if 'ema_min' in stats[key]:\n",
        "    stats[key]['ema_min'] = weighting*(min_val.mean().item()) + (1- weighting) * stats[key]['ema_min']\n",
        "  else:\n",
        "    stats[key]['ema_min'] = weighting*(min_val.mean().item())\n",
        "\n",
        "  if 'ema_max' in stats[key]:\n",
        "    stats[key]['ema_max'] = weighting*(max_val.mean().item()) + (1- weighting) * stats[key]['ema_max']\n",
        "  else: \n",
        "    stats[key]['ema_max'] = weighting*(max_val.mean().item())\n",
        "\n",
        "  stats[key]['min_val'] = stats[key]['min']/ stats[key]['total']\n",
        "  stats[key]['max_val'] = stats[key]['max']/ stats[key]['total']\n",
        "  \n",
        "  return stats\n",
        "\n",
        "# Reworked Forward Pass to access activation Stats through updateStats function\n",
        "def gatherActivationStats(model, x, stats):\n",
        "\n",
        "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv1')\n",
        "  \n",
        "  x = F.relu(model.conv1(x))\n",
        "\n",
        "  x = F.max_pool2d(x, 2, 2)\n",
        "  \n",
        "  stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv2')\n",
        "  \n",
        "  x = F.relu(model.conv2(x))\n",
        "\n",
        "  x = F.max_pool2d(x, 2, 2)\n",
        "\n",
        "  x = x.view(-1, 4*4*50)\n",
        "  \n",
        "  stats = updateStats(x, stats, 'fc1')\n",
        "\n",
        "  x = F.relu(model.fc1(x))\n",
        "  \n",
        "  stats = updateStats(x, stats, 'fc2')\n",
        "\n",
        "  x = model.fc2(x)\n",
        "\n",
        "  return stats\n",
        "\n",
        "# Entry function to get stats of all functions.\n",
        "def gatherStats(model, test_loader):\n",
        "    device = 'cuda'\n",
        "    \n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    stats = {}\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            stats = gatherActivationStats(model, data, stats)\n",
        "    \n",
        "    final_stats = {}\n",
        "    for key, value in stats.items():\n",
        "      final_stats[key] = { \"max\" : value[\"max\"] / value[\"total\"], \"min\" : value[\"min\"] / value[\"total\"], \"ema_min\": value[\"ema_min\"], \"ema_max\": value[\"ema_max\"] }\n",
        "    return final_stats"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBt0WDzyujnk"
      },
      "source": [
        "## Forward Pass for Quantized Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6duGNF_uoZB"
      },
      "source": [
        "def quantForward(model, x, stats, vis=False, axs=None, sym=False, num_bits=8):\n",
        "  X = 0\n",
        "  y = 0\n",
        "  # Quantize before inputting into incoming layers\n",
        "  if sym:\n",
        "    x = quantize_tensor_sym(x, min_val=stats['conv1']['min'], max_val=stats['conv1']['max'], num_bits=num_bits)\n",
        "  else:\n",
        "    x = quantize_tensor(x, min_val=stats['conv1']['min'], max_val=stats['conv1']['max'], num_bits=num_bits)\n",
        "\n",
        "  if vis:\n",
        "    axs[X,y].set_xlabel('Entry into network, input distribution visualised below: ')\n",
        "    # visualise(x.tensor, axs[X,y])\n",
        "  \n",
        "  x, scale_next, zero_point_next = quantizeLayer(x.tensor, model.conv1, stats['conv2'], x.scale, x.zero_point, vis, axs, X=X, y=y+1, sym=sym, num_bits=num_bits)\n",
        "\n",
        "  x = F.max_pool2d(x, 2, 2)\n",
        "  \n",
        "  if vis:\n",
        "    axs[X,y+2].set_xlabel('Output after conv1 visualised below: ')\n",
        "    # visualise(x,axs[X,y+2])\n",
        "\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.conv2, stats['fc1'], scale_next, zero_point_next, vis, axs, X=X, y=y+3, sym=sym, num_bits=num_bits)\n",
        "\n",
        "  x = F.max_pool2d(x, 2, 2)\n",
        "\n",
        "  if vis:\n",
        "    axs[X,y+4].set_xlabel('Output after conv2 visualised below: ')\n",
        "    # visualise(x,axs[X,y+4])\n",
        "\n",
        "  x = x.view(-1, 4*4*50)\n",
        "\n",
        "  x, scale_next, zero_point_next = quantizeLayer(x, model.fc1, stats['fc2'], scale_next, zero_point_next, vis, axs, X=X+1, y=0, sym=sym, num_bits=num_bits)\n",
        "\n",
        "  if vis:\n",
        "    axs[X+1,1].set_xlabel('Output after fc1 visualised below: ')\n",
        "    # visualise(x,axs[X+1,1])\n",
        "  \n",
        "  # Back to dequant for final layer\n",
        "  if sym:\n",
        "    x = dequantize_tensor_sym(QTensor(tensor=x, scale=scale_next, zero_point=zero_point_next))\n",
        "  else:\n",
        "    x = dequantize_tensor(QTensor(tensor=x, scale=scale_next, zero_point=zero_point_next))\n",
        "\n",
        "  if vis:\n",
        "    axs[X+1,2].set_xlabel('Output after fc1 but dequantized visualised below: ')\n",
        "    # visualise(x,axs[X+1,2])\n",
        "\n",
        "  x = model.fc2(x)\n",
        "\n",
        "  if vis:\n",
        "    axs[X+1,3].set_xlabel('Unquantized Weights of fc2 layer')\n",
        "    # visualise(model.fc2.weight.data,axs[X+1,3])\n",
        "\n",
        "    axs[X+1,2].set_xlabel('Output after fc2 but dequantized visualised below: ')\n",
        "    # visualise(x,axs[X+1,4])\n",
        "\n",
        "  return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC96eesMqYo-"
      },
      "source": [
        "# Testing Function for Quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6jKRKSBt0he"
      },
      "source": [
        "def testQuant(model, test_loader, quant=False, stats=None, sym=False, num_bits=8):\n",
        "    device = 'cuda'\n",
        "    \n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            if quant:\n",
        "              output = quantForward(model, data, stats, sym=sym, num_bits=num_bits)\n",
        "            else:\n",
        "              output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs97rNEXt_my"
      },
      "source": [
        "# Get Accuracy of Non Quantized Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YCtbfk9qbGI"
      },
      "source": [
        "import copy\n",
        "q_model = copy.deepcopy(model)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j42Q8PKt3lj"
      },
      "source": [
        "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=64, shuffle=True, **kwargs)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeYlzGG0t4Yp",
        "outputId": "bf7b67c2-f820-483a-ac96-27a913a7d328",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "testQuant(q_model, test_loader, quant=False)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0329, Accuracy: 9891/10000 (99%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JaeISHeuHCb"
      },
      "source": [
        "# Gather Stats of Activations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhiL7OwwuLS6",
        "outputId": "18a3f008-2a2b-4b2f-e2c9-0f2658671b66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "stats = gatherStats(q_model, test_loader)\n",
        "print(stats)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'conv1': {'max': tensor(179.6297, device='cuda:0'), 'min': tensor(-27.0200, device='cuda:0'), 'ema_min': -0.4242129623889923, 'ema_max': 2.819079776573962}, 'conv2': {'max': tensor(568.9222, device='cuda:0'), 'min': tensor(0., device='cuda:0'), 'ema_min': 0.0, 'ema_max': 8.86266755521515}, 'fc1': {'max': tensor(1008.5508, device='cuda:0'), 'min': tensor(0., device='cuda:0'), 'ema_min': 0.0, 'ema_max': 16.084976882368743}, 'fc2': {'max': tensor(543.7311, device='cuda:0'), 'min': tensor(0., device='cuda:0'), 'ema_min': 0.0, 'ema_max': 8.856462522593057}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcxOV-GEy6rD"
      },
      "source": [
        "# Quantisation Aware Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDKoL7ncKeb5"
      },
      "source": [
        "import torch\n",
        "\n",
        "class FakeQuantOp(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, num_bits=8, min_val=None, max_val=None):\n",
        "        x = quantize_tensor(x,num_bits=num_bits, min_val=min_val, max_val=max_val)\n",
        "        x = dequantize_tensor(x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # straight through estimator\n",
        "        return grad_output, None, None, None"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSLtlCSOZ7Ve",
        "outputId": "be2e9bd0-e71c-4953-def7-271e97506673",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = torch.tensor([1,2,3,4]).float()\n",
        "print(FakeQuantOp.apply(x))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 2., 3., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTHK-wAWV57B"
      },
      "source": [
        "## Quantization Aware Training Forward Pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lShb-FGKRp1h"
      },
      "source": [
        "def quantAwareTrainingForward(model, x, stats, vis=False, axs=None, sym=False, num_bits=8, act_quant=False):\n",
        "  \n",
        "  conv1weight = model.conv1.weight.data\n",
        "  model.conv1.weight.data = FakeQuantOp.apply(model.conv1.weight.data, num_bits)\n",
        "  x = F.relu(model.conv1(x))\n",
        "\n",
        "  with torch.no_grad():\n",
        "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv1')\n",
        "\n",
        "  if act_quant:\n",
        "    x = FakeQuantOp.apply(x, num_bits, stats['conv1']['ema_min'], stats['conv1']['ema_max'])\n",
        "\n",
        "  x = F.max_pool2d(x, 2, 2)\n",
        "\n",
        "  conv2weight = model.conv2.weight.data\n",
        "  model.conv2.weight.data = FakeQuantOp.apply(model.conv2.weight.data, num_bits)\n",
        "  x = F.relu(model.conv2(x))\n",
        "\n",
        "  with torch.no_grad():\n",
        "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'conv2')\n",
        "    \n",
        "  if act_quant:\n",
        "    x = FakeQuantOp.apply(x, num_bits, stats['conv2']['ema_min'], stats['conv2']['ema_max'])\n",
        "\n",
        "\n",
        "  x = F.max_pool2d(x, 2, 2)\n",
        "\n",
        "  x = x.view(-1, 4*4*50)\n",
        "\n",
        "  fc1weight = model.fc1.weight.data\n",
        "  model.fc1.weight.data = FakeQuantOp.apply(model.fc1.weight.data, num_bits)\n",
        "  x = F.relu(model.fc1(x))\n",
        "\n",
        "  with torch.no_grad():\n",
        "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'fc1')\n",
        "\n",
        "  if act_quant:\n",
        "    x = FakeQuantOp.apply(x, num_bits, stats['fc1']['ema_min'], stats['fc1']['ema_max'])\n",
        "\n",
        "  x = model.fc2(x)\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    stats = updateStats(x.clone().view(x.shape[0], -1), stats, 'fc2')\n",
        "\n",
        "  return F.log_softmax(x, dim=1), conv1weight, conv2weight, fc1weight, stats"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCyUw4izk41q"
      },
      "source": [
        "# Train using Quantization Aware Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuKYWJfSR65B",
        "outputId": "f604e29d-6e3a-4da0-99e9-bf76a9a8ee11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def trainQuantAware(args, model, device, train_loader, optimizer, epoch, stats, act_quant=False, num_bits=4):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output, conv1weight, conv2weight, fc1weight, stats = quantAwareTrainingForward(model, data, stats, num_bits=num_bits, act_quant=act_quant)\n",
        "\n",
        "        model.conv1.weight.data = conv1weight\n",
        "        model.conv2.weight.data = conv2weight\n",
        "        model.fc1.weight.data = fc1weight\n",
        "\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % args[\"log_interval\"] == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return stats\n",
        "\n",
        "def testQuantAware(args, model, device, test_loader, stats, act_quant, num_bits=4):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output, conv1weight, conv2weight, fc1weight, _ = quantAwareTrainingForward(model, data, stats, num_bits=num_bits, act_quant=act_quant)\n",
        "            \n",
        "            model.conv1.weight.data = conv1weight\n",
        "            model.conv2.weight.data = conv2weight\n",
        "            model.fc1.weight.data = fc1weight\n",
        "\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "def mainQuantAware(mnist=True):\n",
        " \n",
        "    batch_size = 64\n",
        "    test_batch_size = 64\n",
        "    epochs = 10\n",
        "    lr = 0.01\n",
        "    momentum = 0.5\n",
        "    seed = 1\n",
        "    log_interval = 500\n",
        "    save_model = False\n",
        "    no_cuda = False\n",
        "    \n",
        "    use_cuda = not no_cuda and torch.cuda.is_available()\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "    if mnist:\n",
        "      train_loader = torch.utils.data.DataLoader(\n",
        "          datasets.MNIST('../data', train=True, download=True,\n",
        "                        transform=transforms.Compose([\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize((0.1307,), (0.3081,))\n",
        "                        ])),\n",
        "          batch_size=batch_size, shuffle=True, **kwargs)\n",
        "      \n",
        "      test_loader = torch.utils.data.DataLoader(\n",
        "          datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize((0.1307,), (0.3081,))\n",
        "                        ])),\n",
        "          batch_size=test_batch_size, shuffle=True, **kwargs)\n",
        "    else:\n",
        "      transform = transforms.Compose(\n",
        "          [transforms.ToTensor(),\n",
        "          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "      trainset = datasets.CIFAR10(root='./dataCifar', train=True,\n",
        "                                              download=True, transform=transform)\n",
        "      train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                                shuffle=True, num_workers=2)\n",
        "\n",
        "      testset = datasets.CIFAR10(root='./dataCifar', train=False,\n",
        "                                            download=True, transform=transform)\n",
        "      test_loader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size,\n",
        "                                              shuffle=False, num_workers=2)\n",
        "          \n",
        "  \n",
        "    model = Net(mnist=mnist).to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "    args = {}\n",
        "    args[\"log_interval\"] = log_interval\n",
        "    epochs = 10\n",
        "    num_bits=4\n",
        "    stats = {}\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        if epoch > 5:\n",
        "          act_quant = True \n",
        "        else:\n",
        "          act_quant = False\n",
        "\n",
        "        stats = trainQuantAware(args, model, device, train_loader, optimizer, epoch, stats, act_quant, num_bits=num_bits)\n",
        "        testQuantAware(args, model, device, test_loader, stats, act_quant, num_bits=num_bits)\n",
        "\n",
        "    if (save_model):\n",
        "        torch.save(model.state_dict(),\"mnist_cnn.pt\")\n",
        "\n",
        "    return model, stats\n",
        "\n",
        "model, old_stats = mainQuantAware()\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.298113\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.153842\n",
            "\n",
            "Test set: Average loss: 0.1074, Accuracy: 9668/10000 (97%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.217577\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.165507\n",
            "\n",
            "Test set: Average loss: 0.0679, Accuracy: 9790/10000 (98%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.029324\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.070321\n",
            "\n",
            "Test set: Average loss: 0.0491, Accuracy: 9840/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.023828\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.034330\n",
            "\n",
            "Test set: Average loss: 0.0417, Accuracy: 9861/10000 (99%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.099837\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.036195\n",
            "\n",
            "Test set: Average loss: 0.0385, Accuracy: 9873/10000 (99%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.083379\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.012269\n",
            "\n",
            "Test set: Average loss: 0.0426, Accuracy: 9861/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.015122\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.028069\n",
            "\n",
            "Test set: Average loss: 0.0351, Accuracy: 9888/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.040520\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.003336\n",
            "\n",
            "Test set: Average loss: 0.0448, Accuracy: 9851/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.011969\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.035080\n",
            "\n",
            "Test set: Average loss: 0.0300, Accuracy: 9897/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.009853\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.052197\n",
            "\n",
            "Test set: Average loss: 0.0349, Accuracy: 9886/10000 (99%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHGUF12BlAVW"
      },
      "source": [
        "# Test Quantization Aware Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOisGIdHthIt"
      },
      "source": [
        "def testQuantAware(model, test_loader, stats=None, sym=False, num_bits=4):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output, conv1weight, conv2weight, fc1weight, _ = quantAwareTrainingForward(model, data, stats, num_bits=num_bits, act_quant=True, sym=False)\n",
        "            \n",
        "            model.conv1.weight.data = conv1weight\n",
        "            model.conv2.weight.data = conv2weight\n",
        "            model.fc1.weight.data = fc1weight\n",
        "\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeBhP7pZlPln"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgWoRSkNzJVM"
      },
      "source": [
        "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=64, shuffle=True, **kwargs)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJcX_UQYlSUo"
      },
      "source": [
        "## Test Quant Aware "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxmD9WV_l2q2",
        "outputId": "cc6bd568-17f1-4146-9b86-2c1f4262e40c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(old_stats)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'conv1': {'max': tensor(5932740.5000, device='cuda:0'), 'min': tensor(0., device='cuda:0'), 'total': 10950, 'ema_min': 0.0, 'ema_max': 9.396225851789513, 'min_val': tensor(0., device='cuda:0'), 'max_val': tensor(541.8028, device='cuda:0')}, 'conv2': {'max': tensor(9768438., device='cuda:0'), 'min': tensor(0., device='cuda:0'), 'total': 10950, 'ema_min': 0.0, 'ema_max': 16.70547579966727, 'min_val': tensor(0., device='cuda:0'), 'max_val': tensor(892.0948, device='cuda:0')}, 'fc1': {'max': tensor(5035604.5000, device='cuda:0'), 'min': tensor(0., device='cuda:0'), 'total': 10950, 'ema_min': 0.0, 'ema_max': 9.233846474339742, 'min_val': tensor(0., device='cuda:0'), 'max_val': tensor(459.8726, device='cuda:0')}, 'fc2': {'max': tensor(9100459., device='cuda:0'), 'min': tensor(-5886817., device='cuda:0'), 'total': 10950, 'ema_min': -9.734129747507147, 'ema_max': 18.339998663942133, 'min_val': tensor(-537.6089, device='cuda:0'), 'max_val': tensor(831.0922, device='cuda:0')}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj756_3-lKYy"
      },
      "source": [
        "import copy\n",
        "q_model = copy.deepcopy(model)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eQC5VimyZF-",
        "outputId": "68a1214c-8fcc-407d-cb50-83b58ec5122f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "testQuantAware(q_model, test_loader, stats=old_stats, sym=False, num_bits=4)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0353, Accuracy: 9877/10000 (99%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2-bit quantization"
      ],
      "metadata": {
        "id": "hogF88i-3FjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainQuantAware(args, model, device, train_loader, optimizer, epoch, stats, act_quant=False, num_bits=4):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output, conv1weight, conv2weight, fc1weight, stats = quantAwareTrainingForward(model, data, stats, num_bits=num_bits, act_quant=act_quant)\n",
        "\n",
        "        model.conv1.weight.data = conv1weight\n",
        "        model.conv2.weight.data = conv2weight\n",
        "        model.fc1.weight.data = fc1weight\n",
        "\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % args[\"log_interval\"] == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return stats\n",
        "\n",
        "def testQuantAware(args, model, device, test_loader, stats, act_quant, num_bits=4):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output, conv1weight, conv2weight, fc1weight, _ = quantAwareTrainingForward(model, data, stats, num_bits=num_bits, act_quant=act_quant)\n",
        "            \n",
        "            model.conv1.weight.data = conv1weight\n",
        "            model.conv2.weight.data = conv2weight\n",
        "            model.fc1.weight.data = fc1weight\n",
        "\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "def mainQuantAware(mnist=True):\n",
        " \n",
        "    batch_size = 64\n",
        "    test_batch_size = 64\n",
        "    epochs = 10\n",
        "    lr = 0.01\n",
        "    momentum = 0.5\n",
        "    seed = 1\n",
        "    log_interval = 500\n",
        "    save_model = False\n",
        "    no_cuda = False\n",
        "    \n",
        "    use_cuda = not no_cuda and torch.cuda.is_available()\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "    if mnist:\n",
        "      train_loader = torch.utils.data.DataLoader(\n",
        "          datasets.MNIST('../data', train=True, download=True,\n",
        "                        transform=transforms.Compose([\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize((0.1307,), (0.3081,))\n",
        "                        ])),\n",
        "          batch_size=batch_size, shuffle=True, **kwargs)\n",
        "      \n",
        "      test_loader = torch.utils.data.DataLoader(\n",
        "          datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize((0.1307,), (0.3081,))\n",
        "                        ])),\n",
        "          batch_size=test_batch_size, shuffle=True, **kwargs)\n",
        "    else:\n",
        "      transform = transforms.Compose(\n",
        "          [transforms.ToTensor(),\n",
        "          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "      trainset = datasets.CIFAR10(root='./dataCifar', train=True,\n",
        "                                              download=True, transform=transform)\n",
        "      train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                                shuffle=True, num_workers=2)\n",
        "\n",
        "      testset = datasets.CIFAR10(root='./dataCifar', train=False,\n",
        "                                            download=True, transform=transform)\n",
        "      test_loader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size,\n",
        "                                              shuffle=False, num_workers=2)\n",
        "          \n",
        "  \n",
        "    model = Net(mnist=mnist).to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "    args = {}\n",
        "    args[\"log_interval\"] = log_interval\n",
        "    epochs = 10\n",
        "    num_bits=2\n",
        "    stats = {}\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        if epoch > 5:\n",
        "          act_quant = True \n",
        "        else:\n",
        "          act_quant = False\n",
        "\n",
        "        stats = trainQuantAware(args, model, device, train_loader, optimizer, epoch, stats, act_quant, num_bits=num_bits)\n",
        "        testQuantAware(args, model, device, test_loader, stats, act_quant, num_bits=num_bits)\n",
        "\n",
        "    if (save_model):\n",
        "        torch.save(model.state_dict(),\"mnist_cnn.pt\")\n",
        "\n",
        "    return model, stats\n",
        "\n",
        "model, old_stats = mainQuantAware()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8tiD6Uh3Img",
        "outputId": "fce487fc-e6da-4ff0-deea-5ad6ee89ffa8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.297603\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.164514\n",
            "\n",
            "Test set: Average loss: 0.1022, Accuracy: 9688/10000 (97%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.202729\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.147814\n",
            "\n",
            "Test set: Average loss: 0.0852, Accuracy: 9728/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.029570\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.070739\n",
            "\n",
            "Test set: Average loss: 0.0549, Accuracy: 9829/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.032198\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.052849\n",
            "\n",
            "Test set: Average loss: 0.0528, Accuracy: 9831/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.083615\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.039054\n",
            "\n",
            "Test set: Average loss: 0.0513, Accuracy: 9836/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.078763\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.111517\n",
            "\n",
            "Test set: Average loss: 0.0820, Accuracy: 9741/10000 (97%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.044924\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.119605\n",
            "\n",
            "Test set: Average loss: 0.0611, Accuracy: 9794/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.033678\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.019989\n",
            "\n",
            "Test set: Average loss: 0.0919, Accuracy: 9689/10000 (97%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.069761\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.138007\n",
            "\n",
            "Test set: Average loss: 0.0534, Accuracy: 9837/10000 (98%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.048247\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.092768\n",
            "\n",
            "Test set: Average loss: 0.0622, Accuracy: 9797/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def testQuantAware(model, test_loader, stats=None, sym=False, num_bits=4):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output, conv1weight, conv2weight, fc1weight, _ = quantAwareTrainingForward(model, data, stats, num_bits=num_bits, act_quant=True, sym=False)\n",
        "            \n",
        "            model.conv1.weight.data = conv1weight\n",
        "            model.conv2.weight.data = conv2weight\n",
        "            model.fc1.weight.data = fc1weight\n",
        "\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "-Y2rG01x5mOZ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "q_model = copy.deepcopy(model)"
      ],
      "metadata": {
        "id": "8kCXK8gf3emv"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testQuantAware(q_model, test_loader, stats=old_stats, sym=True, num_bits=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENakbjk_5E4x",
        "outputId": "bd110979-e6ce-4c52-9aa6-882e429a6816"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0647, Accuracy: 9798/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}